{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6cf8a85",
   "metadata": {},
   "source": [
    " # MACHINE LEARNING PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e5e59",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T22:37:38.442605Z",
     "start_time": "2023-11-29T22:37:37.880980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import all libraries used in the project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46ac34",
   "metadata": {},
   "source": [
    "### 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513a5e1f56ce5b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T23:44:39.989944Z",
     "start_time": "2023-11-29T23:44:39.963080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>release_date</th>\n",
       "      <th>length</th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>...</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>key</th>\n",
       "      <th>mode</th>\n",
       "      <th>uri</th>\n",
       "      <th>release_year</th>\n",
       "      <th>top_year</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Variations on a Polish Theme, Op. 10: No. 5 An...</td>\n",
       "      <td>Szymanowski: Piano Works, Vol. 2</td>\n",
       "      <td>Karol Szymanowski</td>\n",
       "      <td>06/12/1996</td>\n",
       "      <td>76933</td>\n",
       "      <td>53</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.00695</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>70.295</td>\n",
       "      <td>0.238</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:3bcdLMrAxrfn5dxInjIdI2</td>\n",
       "      <td>1996</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Je vous trouve un charme fou - En duo avec Gaë...</td>\n",
       "      <td>Il suffit d'y croire (Version deluxe)</td>\n",
       "      <td>Hoshi</td>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>172626</td>\n",
       "      <td>62</td>\n",
       "      <td>0.6220</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.59900</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>86.976</td>\n",
       "      <td>0.626</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>spotify:track:0C2yaSWVgCUiiqPyYxSOkd</td>\n",
       "      <td>2018</td>\n",
       "      <td>2022</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Me Gusta</td>\n",
       "      <td>On ira où ?</td>\n",
       "      <td>DTF</td>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>175269</td>\n",
       "      <td>72</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.73400</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>89.989</td>\n",
       "      <td>0.356</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:6P3FBaZfUjeWYExU2ShaPZ</td>\n",
       "      <td>2019</td>\n",
       "      <td>2020</td>\n",
       "      <td>gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L’amour en Solitaire</td>\n",
       "      <td>Petite Amie (Deluxe)</td>\n",
       "      <td>Juliette Armanet</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>175266</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.50600</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>128.027</td>\n",
       "      <td>0.539</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:2tn51grfchxArwPXeXkoX5</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goodnight Moon</td>\n",
       "      <td>Volta</td>\n",
       "      <td>Boogie Belgique</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>264735</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.67500</td>\n",
       "      <td>0.711000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>107.993</td>\n",
       "      <td>0.525</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:2rvo9Ddv18aRV0OJldhWTf</td>\n",
       "      <td>2016</td>\n",
       "      <td>2020</td>\n",
       "      <td>alpha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Variations on a Polish Theme, Op. 10: No. 5 An...   \n",
       "1  Je vous trouve un charme fou - En duo avec Gaë...   \n",
       "2                                           Me Gusta   \n",
       "3                               L’amour en Solitaire   \n",
       "4                                     Goodnight Moon   \n",
       "\n",
       "                                   album             artist release_date  \\\n",
       "0       Szymanowski: Piano Works, Vol. 2  Karol Szymanowski   06/12/1996   \n",
       "1  Il suffit d'y croire (Version deluxe)              Hoshi   2018-11-30   \n",
       "2                            On ira où ?                DTF   2019-10-11   \n",
       "3                   Petite Amie (Deluxe)   Juliette Armanet   2018-02-02   \n",
       "4                                  Volta    Boogie Belgique   2016-09-23   \n",
       "\n",
       "   length  popularity  acousticness  danceability   energy  instrumentalness  \\\n",
       "0   76933          53        0.9960         0.329  0.00695          0.866000   \n",
       "1  172626          62        0.6220         0.615  0.59900          0.000008   \n",
       "2  175269          72        0.4130         0.834  0.73400          0.000040   \n",
       "3  175266           0        0.4040         0.797  0.50600          0.000153   \n",
       "4  264735          53        0.0616         0.788  0.67500          0.711000   \n",
       "\n",
       "   ...  speechiness    tempo  valence  time_signature  key  mode  \\\n",
       "0  ...       0.0448   70.295    0.238               4   11     0   \n",
       "1  ...       0.2530   86.976    0.626               4    1     1   \n",
       "2  ...       0.3410   89.989    0.356               4    6     0   \n",
       "3  ...       0.0327  128.027    0.539               4    5     0   \n",
       "4  ...       0.0318  107.993    0.525               4    7     0   \n",
       "\n",
       "                                    uri  release_year top_year     user  \n",
       "0  spotify:track:3bcdLMrAxrfn5dxInjIdI2          1996  unknown  unknown  \n",
       "1  spotify:track:0C2yaSWVgCUiiqPyYxSOkd          2018     2022    delta  \n",
       "2  spotify:track:6P3FBaZfUjeWYExU2ShaPZ          2019     2020    gamma  \n",
       "3  spotify:track:2tn51grfchxArwPXeXkoX5          2018     2018    gamma  \n",
       "4  spotify:track:2rvo9Ddv18aRV0OJldhWTf          2016     2020    alpha  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the mixed_playlist data\n",
    "data = pd.read_csv('C:/Users/User/Downloads/mixed_playlist.csv')\n",
    "# Print the data to check if it has been uploaded correctly\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6999883d4bc17e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T23:44:43.626331Z",
     "start_time": "2023-11-29T23:44:43.567437Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found for alpha in year 2016\n",
      "File not found for alpha in year 2017\n",
      "File not found for alpha in year 2018\n",
      "File not found for alpha in year 2019\n",
      "File not found for alpha in year 2020\n",
      "File not found for alpha in year 2021\n",
      "File not found for alpha in year 2022\n",
      "File not found for beta in year 2016\n",
      "File not found for beta in year 2017\n",
      "File not found for beta in year 2018\n",
      "File not found for beta in year 2019\n",
      "File not found for beta in year 2020\n",
      "File not found for beta in year 2021\n",
      "File not found for beta in year 2022\n",
      "File not found for delta in year 2016\n",
      "File not found for delta in year 2017\n",
      "File not found for delta in year 2018\n",
      "File not found for delta in year 2019\n",
      "File not found for delta in year 2020\n",
      "File not found for delta in year 2021\n",
      "File not found for delta in year 2022\n",
      "File not found for epsilon in year 2016\n",
      "File not found for epsilon in year 2017\n",
      "File not found for epsilon in year 2018\n",
      "File not found for epsilon in year 2019\n",
      "File not found for epsilon in year 2020\n",
      "File not found for epsilon in year 2021\n",
      "File not found for epsilon in year 2022\n",
      "File not found for gamma in year 2016\n",
      "File not found for gamma in year 2017\n",
      "File not found for gamma in year 2018\n",
      "File not found for gamma in year 2019\n",
      "File not found for gamma in year 2020\n",
      "File not found for gamma in year 2021\n",
      "File not found for gamma in year 2022\n",
      "File not found for zeta in year 2016\n",
      "File not found for zeta in year 2017\n",
      "File not found for zeta in year 2018\n",
      "File not found for zeta in year 2019\n",
      "File not found for zeta in year 2020\n",
      "File not found for zeta in year 2021\n",
      "File not found for zeta in year 2022\n"
     ]
    }
   ],
   "source": [
    "#Read the data from each year for each user\n",
    "users = ['alpha', 'beta', 'delta', 'epsilon', 'gamma', 'zeta']\n",
    "\n",
    "# Function to read CSV files for each user and saves the information in a dictionary\n",
    "def read_user_databases(user):\n",
    "    user_data = {}\n",
    "    for year in range(2016, 2023):\n",
    "        file_path = f\"data/recovered_data/user_{user}/user_{user}_{year}.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)  # Read CSV file for each year\n",
    "            user_data[year] = df  # Store DataFrame in a dictionary using year as key\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found for {user} in year {year}\")\n",
    "    return user_data\n",
    "\n",
    "# Read databases for all users\n",
    "all_users_data = {}\n",
    "for user in users:\n",
    "    all_users_data[user] = read_user_databases(user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ad381",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02698c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>key</th>\n",
       "      <th>mode</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.899000e+03</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "      <td>3899.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.438545e+05</td>\n",
       "      <td>31.931521</td>\n",
       "      <td>0.425282</td>\n",
       "      <td>0.554342</td>\n",
       "      <td>0.522426</td>\n",
       "      <td>0.298067</td>\n",
       "      <td>0.167223</td>\n",
       "      <td>-10.454061</td>\n",
       "      <td>0.085069</td>\n",
       "      <td>115.836808</td>\n",
       "      <td>0.384133</td>\n",
       "      <td>3.873044</td>\n",
       "      <td>5.298025</td>\n",
       "      <td>0.467556</td>\n",
       "      <td>2008.102077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.051106e+05</td>\n",
       "      <td>24.938656</td>\n",
       "      <td>0.363239</td>\n",
       "      <td>0.192042</td>\n",
       "      <td>0.265516</td>\n",
       "      <td>0.387045</td>\n",
       "      <td>0.143566</td>\n",
       "      <td>6.221795</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>28.683508</td>\n",
       "      <td>0.254657</td>\n",
       "      <td>0.467562</td>\n",
       "      <td>3.505024</td>\n",
       "      <td>0.499010</td>\n",
       "      <td>97.161336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>-42.117000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.878725e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>-12.837500</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>94.554500</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.241330e+05</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>-8.599000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>115.241000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.743165e+05</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>-6.225000</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>131.984500</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.921683e+06</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.971000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>209.596000</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2022.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length   popularity  acousticness  danceability       energy  \\\n",
       "count  3.899000e+03  3899.000000   3899.000000   3899.000000  3899.000000   \n",
       "mean   2.438545e+05    31.931521      0.425282      0.554342     0.522426   \n",
       "std    1.051106e+05    24.938656      0.363239      0.192042     0.265516   \n",
       "min    0.000000e+00     0.000000      0.000001      0.000000     0.000545   \n",
       "25%    1.878725e+05     1.000000      0.057900      0.424000     0.312000   \n",
       "50%    2.241330e+05    34.000000      0.339000      0.575000     0.547000   \n",
       "75%    2.743165e+05    51.000000      0.793000      0.700000     0.738000   \n",
       "max    1.921683e+06    91.000000      0.996000      0.971000     0.995000   \n",
       "\n",
       "       instrumentalness     liveness     loudness  speechiness        tempo  \\\n",
       "count       3899.000000  3899.000000  3899.000000  3899.000000  3899.000000   \n",
       "mean           0.298067     0.167223   -10.454061     0.085069   115.836808   \n",
       "std            0.387045     0.143566     6.221795     0.092619    28.683508   \n",
       "min            0.000000     0.017900   -42.117000     0.000000     0.000000   \n",
       "25%            0.000004     0.094500   -12.837500     0.036800    94.554500   \n",
       "50%            0.005820     0.112000    -8.599000     0.047000   115.241000   \n",
       "75%            0.774000     0.172000    -6.225000     0.086500   131.984500   \n",
       "max            0.995000     0.991000     0.532000     0.952000   209.596000   \n",
       "\n",
       "           valence  time_signature          key         mode  release_year  \n",
       "count  3899.000000     3899.000000  3899.000000  3899.000000   3899.000000  \n",
       "mean      0.384133        3.873044     5.298025     0.467556   2008.102077  \n",
       "std       0.254657        0.467562     3.505024     0.499010     97.161336  \n",
       "min       0.000000        0.000000     0.000000     0.000000      0.000000  \n",
       "25%       0.160000        4.000000     2.000000     0.000000   2012.000000  \n",
       "50%       0.355000        4.000000     5.000000     0.000000   2016.000000  \n",
       "75%       0.572000        4.000000     8.000000     1.000000   2018.000000  \n",
       "max       0.981000        5.000000    11.000000     1.000000   2022.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Description of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba79b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Index' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Columns name of the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data\u001b[38;5;241m.\u001b[39mcolumns()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Index' object is not callable"
     ]
    }
   ],
   "source": [
    "#Columns name of the data\n",
    "data.columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Info about the data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d666d02",
   "metadata": {},
   "source": [
    "### 4. Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19e050d5214e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T08:44:19.072315Z",
     "start_time": "2023-11-30T08:44:19.067617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total length for each year from 2016 to 2022 across all users\n",
    "for year in range(2016, 2023):\n",
    "    total_length_for_year = 0\n",
    "\n",
    "    for user_data in all_users_data.values():\n",
    "        if year in user_data:\n",
    "            total_length_for_year += len(user_data[year])\n",
    "\n",
    "    print(f\"Total length of all DataFrames for year {year}: {total_length_for_year}\")\n",
    "print(f\"Total length of the mixed_playlist: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293c9e69ce86cc5",
   "metadata": {},
   "source": [
    "As we can see there is in total 3799 data across all users, so this means there are 100 songs that need to be assigned to one user and one year. Also it is important to check if there is any repeated data for the recovered_data for each user and for every year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc643c588823821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:58:13.488275Z",
     "start_time": "2023-11-30T11:58:13.486326Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Generate combinations of all databases\n",
    "databases = list(all_users_data.values())\n",
    "combinations = list(itertools.combinations(databases, 2))  # Generate all possible combinations of 2 databases\n",
    "\n",
    "columns_to_compare = ['name', 'album']  # Adjust this list to include the columns you want to compare\n",
    "\n",
    "all_different = True\n",
    "\n",
    "# Iterate through combinations and check if selected columns in DataFrames are equal for any pair of databases\n",
    "for combination in combinations:\n",
    "    db1 = combination[0]\n",
    "    db2 = combination[1]\n",
    "\n",
    "    for year in range(2016, 2023):\n",
    "        for user in db1.keys():\n",
    "            if year in db1[user] and year in db2[user]:\n",
    "                for col in columns_to_compare:\n",
    "                    if not db1[user][year][col].equals(db2[user][year][col]):\n",
    "                        all_different = False\n",
    "                        break  # Exit the loop if any pair of selected columns are found to be equal\n",
    "                if not all_different:\n",
    "                    break  # Exit the loop if any pair of selected columns are found to be equal\n",
    "            else:\n",
    "                break\n",
    "        if not all_different:\n",
    "            break  # Exit the loop if any pair of selected columns are found to be equal\n",
    "\n",
    "# Print the result\n",
    "print(f\"All selected columns across databases are different: {all_different}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48036cae011d7e15",
   "metadata": {},
   "source": [
    "As we can see, all the values are different por all data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NA values \n",
    "data.dropna(inplace=True)\n",
    "# Remove duplicates for the mixed_playlist\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the correlation matrix between numeric variables\n",
    "corr = data.corr(numeric_only = True)\n",
    "corr.style.background_gradient(cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03319b0",
   "metadata": {},
   "source": [
    "There is strong correlation between loudness and energy, so fo this case loudness will be removed from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the  loudness feature \n",
    "data.drop(columns = 'loudness', inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove extreme outliers\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define a function to remove outliers based on IQR\n",
    "def remove_outliers(col):\n",
    "    global data\n",
    "    lower_bound = Q1[col] - 1.5 * IQR[col]\n",
    "    upper_bound = Q3[col] + 1.5 * IQR[col]\n",
    "    data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]\n",
    "\n",
    "\n",
    "# Remove outliers from each numerical column\n",
    "for col in columns_to_scale:\n",
    "    remove_outliers(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9a414",
   "metadata": {},
   "source": [
    "Now, we are going separate the known and unknown data in two diferent dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with 'unknown' in the 'user' column\n",
    "unkown_data= data[data['user'] == 'unknown']\n",
    "known_data = data[data['user'] != 'unknown']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69870b2b",
   "metadata": {},
   "source": [
    "In this case, a new column will be created and this will merge the 'user' and 'top_year' columns, to have all possible targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251efb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_data['user'].fillna('', inplace=True)  # Replace NaN values in 'user' column with empty string\n",
    "known_data['top_year'].fillna('', inplace=True)  # Replace NaN values in 'top_year' column with empty string\n",
    "\n",
    "known_data['user_year'] = known_data['user'].str.cat(known_data['top_year'], sep=' ')\n",
    "known_data = known_data.drop(columns=['user', 'top_year'])\n",
    "known_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da602b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = known_data['user_year'].unique()\n",
    "\n",
    "# Create an empty dictionary to store mapping of unique values to categorical values\n",
    "mapping = {}\n",
    "\n",
    "# Assign categorical values to unique values using a for loop\n",
    "for idx, val in enumerate(unique_values):\n",
    "    mapping[val] = idx + 1  # Assign categorical values starting from 1\n",
    "\n",
    "# Map the 'user_year' column to categorical values using the mapping dictionary\n",
    "known_data['target'] = known_data['user_year'].map(mapping)\n",
    "known_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fea46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = known_data['target']\n",
    "feature_df = known_data.drop(columns = ['name', 'album', 'artist','release_date','uri','user_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into testing and training part\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_df.values,\n",
    "    target_df.values,\n",
    "    train_size = 0.7,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b687a",
   "metadata": {},
   "source": [
    "### 5. Apply models to see which one fits the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d9692",
   "metadata": {},
   "source": [
    "The goal is to categorize input data into predefined categories. This is then a classification problem and so we will see classification algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678bd651",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into testing and training part\n",
    "X_train_mlr, X_test_mlr, y_train_mlr, y_test_mlr = train_test_split(\n",
    "    feature_df.values,\n",
    "    target_df.values,\n",
    "    train_size = 0.7,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5246147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of features:\", len(feature_df.columns))\n",
    "print(\"Size of training dataset:\", len(X_train_mlr))\n",
    "print(\"Size of testing dataset:\", len(X_test_mlr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mlr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a00fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = LinearRegression(fit_intercept = True)\n",
    "mlr.fit(X_train_mlr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c362336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the learned parameter values\n",
    "parameter = pd.Series(\n",
    "    data = mlr.coef_,\n",
    "    index = feature_df.columns + '_coef'\n",
    ")\n",
    "parameter['intercept'] = mlr.intercept_\n",
    "print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance on the training dataset\n",
    "rmse_train = mean_squared_error(y_train, mlr.predict(X_train), squared = False)\n",
    "r2_train = r2_score(y_train, mlr.predict(X_train))\n",
    "\n",
    "print(\"RMSE train:\", rmse_train)\n",
    "print(\"R2 train:\", r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plt.figure(figsize = [5, 4])\n",
    "plt.plot(y_train, mlr.predict(X_train), '.', label = 'Train')\n",
    "plt.plot(y_train, y_train, '-', label = '1:1 line')\n",
    "plt.legend()\n",
    "plt.xlabel('target value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b3f8c",
   "metadata": {},
   "source": [
    "## MULTI-CLASS LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fff439",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_lr = {\n",
    "    'lr__C': [0.001, 0.01, 0.1, 1, 10],  # Regularization parameter\n",
    "    'lr__penalty': ['l1', 'l2'],  # Type of regularization used\n",
    "    'lr__solver': ['liblinear', 'saga'],  # Solvers that support both l1 and l2\n",
    "    'select__k': range(6, 15),  # Parameter for SelectKBest\n",
    "}\n",
    "\n",
    "clf_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_regression)),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_lr = GridSearchCV(\n",
    "    estimator=clf_lr,\n",
    "    param_grid=param_dict_lr,\n",
    "    scoring='f1_weighted',\n",
    "    refit=True,\n",
    "    cv=3,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "grid_clf_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict categories for testing dataset LR\n",
    "y_pred_lr = grid_clf_lr.predict(X_test)\n",
    "y_pred_proba_lr = grid_clf_lr.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and visualize the confusion matrix LR\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "disp_lr = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = cm_lr,\n",
    "    display_labels = unique_values\n",
    ")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb497b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report Multi-Class Logistic Regression : \")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve LR\n",
    "fpr_lr, tpr_lr, thresholds_;r = roc_curve(\n",
    "    y_test, # the true class\n",
    "    y_pred_proba_lr[:, 1], # the second column, the probability of being positive\n",
    "    pos_label = 1 # the positive class is labeled as 1\n",
    ")\n",
    "\n",
    "# Plot ROC curve LR\n",
    "plt.figure(figsize = [5, 4])\n",
    "plt.plot(fpr_lr, tpr_lr, '-')\n",
    "plt.xlabel('False Positive Rate LR')\n",
    "plt.ylabel('True Positive Rate LR')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072940d9",
   "metadata": {},
   "source": [
    "## K NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_knn = {\n",
    "    'knn__n_neighbors': range(3,32,2),\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'select__k': range(2,15),  # Parameter for SelectKBest\n",
    "}\n",
    "\n",
    "# construct the pipeline\n",
    "clf_knn = Pipeline([\n",
    "    ('standard scaler', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_regression)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation KNN\n",
    "max_iter=999\n",
    "grid_clf_knn = RandomizedSearchCV(\n",
    "    estimator=clf_knn,\n",
    "    param_distributions=param_dict_knn,\n",
    "    scoring='f1_weighted',\n",
    "    n_iter=max_iter,\n",
    "    refit=True,\n",
    "    cv=3,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "grid_clf_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18146b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best estimator and the best score\n",
    "print(\"Result KNN: \")\n",
    "print('Best hyper-parameters KNN:', grid_clf_knn.best_params_)\n",
    "print('Best score KNN:', grid_clf_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for testing dataset\n",
    "y_pred_proba = clf_knn.predict_proba(X_test)\n",
    "# show the first 5 predictions\n",
    "y_pred_proba[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a963bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict categories for testing dataset KNN\n",
    "y_pred_knn = grid_clf_knn.predict(X_test)\n",
    "y_pred_proba_knn = grid_clf_knn.predict_proba(X_test)\n",
    "\n",
    "# compute and visualize the confusion matrix KNN\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = cm_knn,\n",
    "    display_labels = unique_values\n",
    ")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95be926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all metrics using `classification_report`\n",
    "print(\"Classification report KNN : \")\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve KNN\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(\n",
    "    y_test, # the true class\n",
    "    y_pred_proba_knn[:, 1], # the second column, the probability of being positive\n",
    "    pos_label = 1 # the positive class is labeled as 1\n",
    ")\n",
    "\n",
    "# Plot ROC curve DT\n",
    "plt.figure(figsize = [5, 4])\n",
    "plt.plot(fpr_knn, tpr_knn, '-')\n",
    "plt.xlabel('False Positive Rate KNN')\n",
    "plt.ylabel('True Positive Rate KNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922c992",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54904f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_svc = {\n",
    "    'svc__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf', 'poly','sigmoid'],\n",
    "    'select__k': range(6,15),  # Parameter for SelectKBest\n",
    "}\n",
    "\n",
    "clf_svc = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_regression)),\n",
    "    ('svc', SVC())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85aaf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_svc = GridSearchCV(\n",
    "    estimator=clf_svc,\n",
    "    param_grid=param_dict_svc,\n",
    "    scoring='f1_weighted',\n",
    "    refit=True,\n",
    "    cv=3,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "grid_clf_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict categories for testing dataset SVC\n",
    "y_pred_svc = grid_clf_svc.predict(X_test)\n",
    "cm_svc = confusion_matrix(y_test, y_pred_svc)\n",
    "disp_svc = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = cm_svc,\n",
    "    display_labels = unique_values\n",
    ")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report SVC : \")\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve SVC\n",
    "fpr_svc, tpr_svc, thresholds_svc = roc_curve(\n",
    "    y_test, # the true class\n",
    "    y_pred_proba_svc[:, 1], # the second column, the probability of being positive\n",
    "    pos_label = 1 # the positive class is labeled as 1\n",
    ")\n",
    "\n",
    "# Plot ROC curve SVC\n",
    "plt.figure(figsize = [5, 4])\n",
    "plt.plot(fpr_svc tpr_svc, '-')\n",
    "plt.xlabel('False Positive Rate SVC')\n",
    "plt.ylabel('True Positive Rate SVC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906cb9ec",
   "metadata": {},
   "source": [
    "## DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_dt = {\n",
    "    'dt__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'dt__max_depth': range(1,30,2),\n",
    "    'dt__min_samples_split': range(1,20),\n",
    "    'dt__min_samples_leaf' :range(1,10),\n",
    "    'dt__max_features': [\"auto\", \"sqrt\", \"log2\"],\n",
    "    'dt__min_impurity_decrease':[0.0,0.1,0.2],\n",
    "    'select__k': range(6, 15)  # Paramètre pour SelectKBest\n",
    "}\n",
    "\n",
    "clf_dt = Pipeline([\n",
    "    ('select', SelectKBest(score_func=f_regression)),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_dt = RandomizedSearchCV(\n",
    "    estimator=clf_dt,\n",
    "    param_distributions=param_dict_dt,\n",
    "    scoring='f1_weighted',\n",
    "    n_iter=max_iter,\n",
    "    refit=True,\n",
    "    cv=3,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "grid_clf_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict categories for testing dataset DT\n",
    "y_pred_dt = grid_clf_dt.predict(X_test)\n",
    "# compute and visualize the confusion matrix DT\n",
    "cm_dt = confusion_matrix(y_test, y_pred)\n",
    "disp_dt = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = cm_dt,\n",
    "    display_labels = ['2016','2017','2018','2019','2020','2021','2022']\n",
    ")\n",
    "disp_dt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ef330",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report Decision Trees : \")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2db957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve DT\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(\n",
    "    y_test, # the true class\n",
    "    y_pred_proba_dt[:, 1], # the second column, the probability of being positive\n",
    "    pos_label = 1 # the positive class is labeled as 1\n",
    ")\n",
    "\n",
    "# Plot ROC curve DT\n",
    "plt.figure(figsize = [5, 4])\n",
    "plt.plot(fpr_dt, tpr_dt, '-')\n",
    "plt.xlabel('False Positive Rate DT')\n",
    "plt.ylabel('True Positive Rate DT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97443993",
   "metadata": {},
   "source": [
    "## RANDOM FOREST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_rf = {\n",
    "    'rf__n_estimators': [10, 50, 100, 200],\n",
    "    'rf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'rf__max_depth': range(5, 15),\n",
    "    'rf__criterion': ['gini', 'entropy'],\n",
    "    'select__k': range(1,15),  # Parameter for SelectKBest\n",
    "}\n",
    "\n",
    "clf_rf = Pipeline([\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('select', SelectKBest(score_func=f_regression))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd28322",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_rf = RandomizedSearchCV(\n",
    "    estimator=clf_rf,\n",
    "    param_distributions=param_dict_rf,\n",
    "    scoring='f1_weighted',\n",
    "    n_iter=max_iter,\n",
    "    refit=True,\n",
    "    cv=3,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "grid_clf_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict categories for testing dataset DT\n",
    "y_pred_dt = grid_clf_dt.predict(X_test)\n",
    "# compute and visualize the confusion matrix DT\n",
    "cm_dt = confusion_matrix(y_test, y_pred)\n",
    "disp_dt = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = cm_dt,\n",
    "    display_labels = ['2016','2017','2018','2019','2020','2021','2022']\n",
    ")\n",
    "disp_dt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220268f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report Decision Trees : \")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bdf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve RF\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(\n",
    "    y_test, # the true class\n",
    "    y_pred_proba_rf[:, 1], # the second column, the probability of being positive\n",
    "    pos_label = 1 # the positive class is labeled as 1\n",
    ")\n",
    "\n",
    "# Plot ROC curve RF\n",
    "plt.figure(figsize = [5, 4])\n",
    "plt.plot(fpr_rf, tpr_rf, '-')\n",
    "plt.xlabel('False Positive Rate RF')\n",
    "plt.ylabel('True Positive Rate RF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f31af0",
   "metadata": {},
   "source": [
    "## GRADIENT BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc076db",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_gb = {\n",
    "    'gb__n_estimators': [100, 200, 300],\n",
    "    'gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gb__max_depth': range(3, 10),\n",
    "    'select__k': range(1,15),  # Parameter for SelectKBest\n",
    "}\n",
    "\n",
    "\n",
    "clf_gb = Pipeline([\n",
    "    ('gb', GradientBoostingClassifier()),\n",
    "    ('select', SelectKBest(score_func=f_regression)),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e08335",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_gb = RandomizedSearchCV(\n",
    "    estimator=clf_gb,\n",
    "    param_distributions=param_dict_gb,\n",
    "    scoring='f1_weighted',\n",
    "    n_iter=max_iter,\n",
    "    refit=True,\n",
    "    cv=3,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "grid_clf_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd143a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict categories for testing dataset GB\n",
    "y_pred_gb = grid_clf_gb.predict(X_test)\n",
    "# compute and visualize the confusion matrix GB\n",
    "cm_gb = confusion_matrix(y_test, y_pred)\n",
    "disp_gb = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = cm_gb,\n",
    "    display_labels = ['2016','2017','2018','2019','2020','2021','2022']\n",
    ")\n",
    "disp_gb.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd919631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report Gradient Boost : \")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d498a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve GB\n",
    "fpr_gb, tpr_gb, thresholds_gb = roc_curve(\n",
    "    y_test, # the true class\n",
    "    y_pred_proba_gb[:, 1], # the second column, the probability of being positive\n",
    "    pos_label = 1 # the positive class is labeled as 1\n",
    ")\n",
    "\n",
    "# Plot ROC curve GB\n",
    "plt.figure(figsize = [5, 4])\n",
    "plt.plot(fpr_gb, tpr_gb, '-')\n",
    "plt.xlabel('False Positive Rate GB')\n",
    "plt.ylabel('True Positive Rate GB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e4842",
   "metadata": {},
   "source": [
    "## NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict_nb = {\n",
    "    'nb__var_smoothing': np.logspace(-9, -1, 100),\n",
    "    'select__k': range(1,15),  # Parameter for SelectKBest\n",
    "}\n",
    "\n",
    "clf_nb = Pipeline([\n",
    "    ('nb', GaussianNB()),\n",
    "    ('select', SelectKBest(score_func=f_regression))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccde9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_nb = RandomizedSearchCV(\n",
    "    estimator=clf_nb,\n",
    "    param_distributions=param_dict_nb,\n",
    "    scoring='f1_weighted',\n",
    "    n_iter=max_iter,\n",
    "    refit=True,\n",
    "    cv=3,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "grid_clf_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict categories for testing dataset NB\n",
    "y_pred_nb = grid_clf_nb.predict(X_test)\n",
    "# compute and visualize the confusion matrix NB\n",
    "cm_nb = confusion_matrix(y_test, y_pred)\n",
    "disp_nb = ConfusionMatrixDisplay(\n",
    "    confusion_matrix = cm_nb,\n",
    "    display_labels = ['2016','2017','2018','2019','2020','2021','2022']\n",
    ")\n",
    "disp_nb.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report Naive Bayes : \")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da04b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve NB\n",
    "fpr_nb, tpr_nb, thresholds_nb = roc_curve(\n",
    "    y_test, # the true class\n",
    "    y_pred_proba_nb[:, 1], # the second column, the probability of being positive\n",
    "    pos_label = 1 # the positive class is labeled as 1\n",
    ")\n",
    "\n",
    "# Plot ROC curve NB\n",
    "plt.figure(figsize = [5, 4])\n",
    "plt.plot(fpr_nb, tpr_nb, '-')\n",
    "plt.xlabel('False Positive Rate NB')\n",
    "plt.ylabel('True Positive Rate NB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abf32d",
   "metadata": {},
   "source": [
    "We foudn that the most efficient algortihm is : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de5369",
   "metadata": {},
   "source": [
    "## 6.  Reconstructing playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff731f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "mixed_playlist = pd.read_csv('C:/Users/User/Downloads/mixed_playlist.csv')\n",
    "\n",
    "# Apply the previously fitted scaler on the new data\n",
    "scaled_features = scaler.transform(mixed_playlist[['length', 'popularity', 'acousticness', 'danceability', 'energy', 'instrumentalness',\n",
    "                    'liveness', 'speechiness', 'tempo', 'valence', 'time_signature',\n",
    "                    'key', 'mode', 'release_year']])\n",
    "\n",
    "mixed_playlist_scaled = mixed_playlist.copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "mixed_playlist_scaled[columns_to_scale] = scaler.fit_transform(mixed_playlist[columns_to_scale])\n",
    "\n",
    "# Predict using the models with the scaled features\n",
    "predicted_top_year = grid_clf_knn.predict(scaled_features)\n",
    "predicted_user = grid_clf_knn.predict(scaled_features)\n",
    "\n",
    "# Add the predicted genres to the dataset\n",
    "mixed_playlist['predicted_top_year'] = predicted_top_year\n",
    "mixed_playlist['predicted_user'] = predicted_user\n",
    "\n",
    "# Save the updated DataFrame with predictions to a new CSV file\n",
    "mixed_playlist.to_csv('C:/Users/User/Downloads/mixed_playlist_with_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
